{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simulating Sparse DNNs on initialization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcVdoiOQKjTW/x4Fv/YCV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcnica89/DNNs/blob/main/Simulating_Sparse_DNNs_on_initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OscYo56EM9PF"
      },
      "outputs": [],
      "source": [
        "#Import packages we use!  mostly jax (which is like numpy but beefed up)\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "from jax import nn\n",
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd  #data frames for use in plotting\n",
        "from plotnine import * #this is the ggplot package!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#These helper functions involve the L2 norm ||v||^2 of a vector, \n",
        "# BUT work assuming that you have many vectors in a single array, so that \n",
        "# v is of shape (dim, N_samples) where dim is the dimension and N_samples is the number of samples \n",
        "def norm2(v):\n",
        "  return jnp.einsum(\"is,is->s\",v,v)\n",
        "\n",
        "def norm(v):\n",
        " return jnp.sqrt(norm2(v))\n",
        "\n",
        "def unit_vector(v):\n",
        "  return v/norm(v)"
      ],
      "metadata": {
        "id": "5wCEwYqoNCZx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating a simple fully connected network"
      ],
      "metadata": {
        "id": "KTEz9LWqe6uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_width = 2**6 #network width = number of neurons per layer\n",
        "N_depth = 2**6 #network depth = number of layers\n",
        "N_samples =  int(2**32/(N_width**2 * N_depth)) #number of samples to run simulatenously\n",
        "#(we use a number of samples that grows/shrinks with depth and width so that we can automatically fill the computers memory with samples)\n",
        "\n",
        "key = random.PRNGKey(int(time.time())) #random key for generating random numbers\n",
        "  \n",
        "#Initial input vector is a random unit vector\n",
        "input = unit_vector(random.normal(key,(N_width,N_samples), dtype=jnp.float64))\n",
        "keys = random.split(key, N_depth) #get a random key used to generate each layer of the network\n",
        "\n",
        "z = input\n",
        "for layer in range(N_depth): \n",
        "  #Setup the weight matrix and normalize by the fan-in\n",
        "  W = random.normal(keys[layer],(N_width,N_width,N_samples),dtype=jnp.float64)*math.sqrt(2/N_width)\n",
        "\n",
        "  phi = nn.relu(z) #vector after applying the activation function\n",
        "  z = jnp.einsum(\"ijs,js->is\",W,phi) #Apply the weight matrix W in each sample\n",
        "\n",
        "output = z #reutrn a vector of shape (N_width, N_samples) with the outputs!"
      ],
      "metadata": {
        "id": "DzT2O8QvNt9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_norm2 = norm2(output) #take the norm of each simulation to get a vector of shape (N_samples,) with the output\n",
        "print(f'Mean norm2 of output is {jnp.mean(output_norm2)} using {N_samples} samples')\n",
        "print(f'Var norm2 of output is {jnp.var(output_norm2)} using {N_samples} samples')\n",
        "ln_output_norm2 = jnp.log(output_norm2)\n",
        "print(f'Mean ln(norm2) of output is {jnp.mean(ln_output_norm2)} using {N_samples} samples')\n",
        "print(f'Var ln(norm2) of output is {jnp.var(ln_output_norm2)} using {N_samples} samples')\n",
        "\n",
        "print(f'Var + 2*Mean of ln(norm2) is {jnp.var(ln_output_norm2)+2*jnp.mean(ln_output_norm2)}') #This should be close to 0!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi9_9JdzsgfR",
        "outputId": "b4ecd937-a7c2-4882-fa74-b12fcb55fe66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean norm2 of output is 0.9477857351303101 using 16384 samples\n",
            "Var norm2 of output is 38.29972839355469 using 16384 samples\n",
            "Mean ln(norm2) of output is -2.565073013305664 using 16384 samples\n",
            "Var ln(norm2) of output is 5.311448574066162 using 16384 samples\n",
            "Var + 2*Mean of ln(norm2) is 0.18130254745483398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the output with ggplot"
      ],
      "metadata": {
        "id": "EJCvA11hPeQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#def NormalCDF(mean,var,x):\n",
        "#  return 0.5*math.erfc(-(x-mean)/math.sqrt(2*var))\n",
        "\n",
        "#def NormalPDF(mean,var,x):\n",
        "#  return math.exp(-(x-mean)**2/(2*var))/math.sqrt(2*math.pi*var)\n",
        "\n",
        "\n",
        "#df = pd.DataFrame(jnp.log(output_norm2),columns=['ln_norm2']) #load data into a DataFrame object\n",
        "#plot = (\n",
        "#       ggplot(df, aes(x='ln_norm2')) +\n",
        "#       labs(title='Histogram of log(norm2(output))') +\n",
        "#       geom_histogram(aes(y='..density..'), color='black', bins=30) +\n",
        "#       stat_function(fun=lambda x: NormalPDF(-2.5*N_depth/N_width,5*N_depth/N_width,x), color='red')\n",
        "#)"
      ],
      "metadata": {
        "id": "KhTK4xRwPc9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(plot)"
      ],
      "metadata": {
        "id": "aeS4fZlFT6y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Networks"
      ],
      "metadata": {
        "id": "CIAIE2a7cnmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_sparseDNNs(key,N_per_neuron,N_width,N_depth,N_samples,sparse_type='bernoulli'):\n",
        "  '''Simulate sparse DNNs that have N_per_neuron connections per neuron (on average)'''\n",
        "  \n",
        "  #Initial input vector is a random unit vector\n",
        "  input = unit_vector(random.normal(key,(N_width,N_samples), dtype=jnp.float64))\n",
        "  keys = random.split(key, 2*N_depth) #get a random key used to generate each layer of the network\n",
        "\n",
        "  z = input\n",
        "  for layer in range(N_depth): \n",
        "  \n",
        "    #Setup the masks for the sparsity\n",
        "    if sparse_type == 'const-fan-in':\n",
        "      M = jnp.concatenate( (jnp.ones((N_width, N_per_neuron,N_samples)), jnp.zeros((N_width, N_width-N_per_neuron,N_samples))), axis=1 )\n",
        "      M = random.permutation( keys[2*layer], M, axis=1, independent = True )\n",
        "    elif sparse_type == 'bernoulli':\n",
        "      p = N_per_neuron/N_width #This is the right probability to average N_per_neuron for each neuron\n",
        "      M = random.bernoulli(keys[2*layer], p=p, shape=(N_width,N_width,N_samples))\n",
        "      #REMARK: if N_per_neuron is close to zero, there is chance to get fan-in's=0 to which will mess up the mean 1 property\n",
        "    elif sparse_type == 'const-per-layer':\n",
        "      N_per_layer = N_per_neuron*N_width #Number of neurons in each layer\n",
        "      #....something with random permutation again...\n",
        "      #similar to M = jnp.concatenate( (jnp.ones((N_per_neuron,N_width,N_samples)), jnp.zeros((N_width-N_per_neuron,N_width,N_samples))), axis=0 )\n",
        "      #M = random.permutation( keys[2*layer], M, axis=0, independent = True )\n",
        "\n",
        "    #Calculate the fan-in from the mask and then normalize W accordingly\n",
        "    #fan_in = jnp.sum(M,axis=0) \n",
        "    #fan_in = jnp.where(fan_in == 0, 1, fan_in) #set places with fan_in=0 to 1 to avoid divide by zero error in weight normalization!\n",
        "\n",
        "    #Setup the weight matrix and normalize by the fan-in\n",
        "    W = random.normal(keys[2*layer+1],(N_width,N_width,N_samples),dtype=jnp.float64)*math.sqrt(2/N_per_neuron) #/jnp.sqrt(fan_in) #weight matrices\n",
        "\n",
        "    phi = nn.relu(z) #vector after applying the activation function\n",
        "    z = jnp.einsum(\"ijs,js->is\",M*W,phi) #Apply the weight matrix W in each sample\n",
        "\n",
        "  output = z #vector of shape (N_width, N_samples) with the outputs!\n",
        "  return(output)"
      ],
      "metadata": {
        "id": "8VnXdFaFtkIf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_width = 2**6 #network width = number of neurons per layer\n",
        "N_depth = 1 #network depth = number of layers\n",
        "N_samples = int(2**28/(N_width**2 * N_depth)) #number of samples to run simulatenously\n",
        "N_per_neuron = 12 #2**2\n",
        "sparse_type_list = ['bernoulli','const-fan-in']\n",
        "\n",
        "\n",
        "ber_theory_var = N_depth*(5*N_width - 8 + 18*N_width/N_per_neuron)/N_width/(N_width+2)\n",
        "#print(f'Ber Theory predicts:\\n {ber_theory_var}')\n",
        "cfi_theory_var = ber_theory_var - 3*(N_width - N_per_neuron)/N_width/N_per_neuron/(N_width+2)\n",
        "#print(f'Ber Theory predicts:\\n {cfi_theory_var}')\n",
        "theory = [ber_theory_var, cfi_theory_var]\n",
        "\n",
        "N_trials = 2**3\n",
        "keys = random.split( random.PRNGKey(int(time.time())), N_trials) #random key for generating random numbers\n",
        "\n",
        "output_norm2 = np.zeros((2,N_trials, N_samples))\n",
        "for trial in range(N_trials):\n",
        "  for i,sparse_type in enumerate(sparse_type_list):\n",
        "    output = simulate_sparseDNNs(keys[i], N_per_neuron,N_width,N_depth,N_samples,sparse_type)\n",
        "    output_norm2[i,trial,:] = norm2(output)\n",
        "  \n",
        "print(f'Num simulations={N_trials*N_samples}')\n",
        "print(f'N_width={N_width}')\n",
        "print(f'N_depth={N_depth}')\n",
        "print(f'N_per_neuron={N_per_neuron}')\n",
        "output_ln_norm2 = np.log(output_norm2) #take the norm of each simulation to get a vector of shape (N_samples,) with the output\n",
        "for i,sparse_type in enumerate(sparse_type_list):\n",
        "  print(f'----Results for {sparse_type}-----')\n",
        "  print(f'  Mean norm2:\\n {np.nanmean(output_norm2[i,:,:])}')\n",
        "  var = np.nanvar(output_norm2[i,:,:])\n",
        "  print(f'  Var norm2:\\n {var}')\n",
        "  print(f'  Theory prediction:\\n {theory[i]}')\n",
        "  print(f'  % error:\\n  {100*(theory[i] - var)/var:.2f}%')\n",
        "  #print(f'Var ln(norm2): {np.nanvar(output_ln_norm2[i,:,:])}')\n",
        "  #print(f'Var + 2*Mean of ln(norm2): {np.nanvar(output_ln_norm2[i,:,:])+2*np.mean(output_ln_norm2[i,:,:])}') #This should be close to 0!\n",
        "\n",
        "\n",
        "#ber_theory_var = N_depth*(5*N_width - 8 + 18*N_width/N_per_neuron)/N_width/(N_width+2)\n",
        "#print(f'Ber Theory predicts:\\n {ber_theory_var}')\n",
        "#cfi_theory_var = ber_theory_var - 3*(N_width - N_per_neuron)/N_width/N_per_neuron/(N_width+2)\n",
        "#print(f'Ber Theory predicts:\\n {cfi_theory_var}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMbwJMdCcs5d",
        "outputId": "cb550c91-249a-4586-c985-864693824976"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num simulations=524288\n",
            "N_width=64\n",
            "N_depth=1\n",
            "N_per_neuron=12\n",
            "----Results for bernoulli-----\n",
            "  Mean norm2:\n",
            " 1.0009366690351271\n",
            "  Var norm2:\n",
            " 0.09670697405598401\n",
            "  Theory prediction:\n",
            " 0.09659090909090909\n",
            "  % error:\n",
            "  -0.12%\n",
            "----Results for const-fan-in-----\n",
            "  Mean norm2:\n",
            " 0.9989254558533958\n",
            "  Var norm2:\n",
            " 0.09373992462839252\n",
            "  Theory prediction:\n",
            " 0.09351325757575757\n",
            "  % error:\n",
            "  -0.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Empircal Variance Ratio:\")\n",
        "print(np.nanvar(output_ln_norm2[1,:,:])/np.nanvar(output_ln_norm2[0,:,:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl8Bh3cxTEgQ",
        "outputId": "cf6faea8-4589-4277-c205-10aa4ca36f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empircal Variance Ratio:\n",
            "0.8596238174938919\n",
            "Theoretical Variance Ratio:\n",
            "0.7795918367346939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_width = 50 #2**5 #network width = number of neurons per layer\n",
        "N_depth = 20 #2**5 #network depth = number of layers\n",
        "N_per_neuron = 5 #2**3\n",
        "\n",
        "theory_var = N_depth*(5*N_width - 8 + 18*N_width/N_per_neuron)/N_width/(N_width+2)\n",
        "print(theory_var)\n",
        "print(5*N_depth/ N_width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By1mjrZbCNz8",
        "outputId": "b8a0f606-cb47-4aa0-c92f-022ad1a12a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.246153846153846\n",
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.DataFrame(jnp.log(output_ln_norm2[0,0,:]),columns=['ln_norm2']) #load data into a DataFrame object\n",
        "#plot = (\n",
        "#       ggplot(df, aes(x='ln_norm2')) +\n",
        "#       labs(title='Histogram of log(norm2(output)) BERNOULLI') +\n",
        "#       geom_histogram(aes(y='..density..'), color='black', bins=30)\n",
        "#       #stat_function(fun=lambda x: NormalPDF(-2.5*N_depth/N_width,5*N_depth/N_width,x), color='red')\n",
        "#)"
      ],
      "metadata": {
        "id": "qI3T9omHI-z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.DataFrame(jnp.log(output_ln_norm2[1,0,:]),columns=['ln_norm2']) #load data into a DataFrame object\n",
        "#plot = (\n",
        "#       ggplot(df, aes(x='ln_norm2')) +\n",
        "#       labs(title='Histogram of log(norm2(output))') +\n",
        "#       geom_histogram(aes(y='..density..'), color='black', bins=30)\n",
        "       #stat_function(fun=lambda x: NormalPDF(-2.5*N_depth/N_width,5*N_depth/N_width,x), color='red')\n",
        "#)"
      ],
      "metadata": {
        "id": "e319iZm8ClIM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}